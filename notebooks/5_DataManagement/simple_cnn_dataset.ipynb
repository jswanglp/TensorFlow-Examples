{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"simple_cnn_dataset.ipynb","version":"0.3.2","provenance":[],"collapsed_sections":[],"toc_visible":true},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"GPU"},"cells":[{"metadata":{"id":"5T1hCe9QDzm0","colab_type":"text"},"cell_type":"markdown","source":["# INITIALIZATION"]},{"metadata":{"id":"9gJ_FfKgE9UN","colab_type":"text"},"cell_type":"markdown","source":["- **mount GoogleDrive**"]},{"metadata":{"id":"1YsugnXkESxO","colab_type":"code","colab":{}},"cell_type":"code","source":["from google.colab import drive\n","drive.mount('GoogleDrive')\n"],"execution_count":0,"outputs":[]},{"metadata":{"id":"3Vmiq9aLFOk1","colab_type":"text"},"cell_type":"markdown","source":["- **unmount GoogleDrive**"]},{"metadata":{"id":"1A2EJlwVE3Ku","colab_type":"code","colab":{}},"cell_type":"code","source":["! fusermount -u 'GoogleDrive'"],"execution_count":0,"outputs":[]},{"metadata":{"id":"kM26SWRvFbZd","colab_type":"text"},"cell_type":"markdown","source":["# Code"]},{"metadata":{"id":"T3cqJ1tcFfXq","colab_type":"code","colab":{}},"cell_type":"code","source":["#@title simple_cnn { display-mode: \"both\" }\n","# conding: utf-8\n","import tensorflow as tf\n","from tensorflow.examples.tutorials.mnist import input_data\n","# import tensorflow_datasets as tfds\n","import numpy as np\n","import os"],"execution_count":0,"outputs":[]},{"metadata":{"id":"8p9TxifRF-gD","colab_type":"code","colab":{}},"cell_type":"code","source":["#@title parameter settings { display-mode: \"both\" }\n","\n","tf.logging.set_verbosity(tf.logging.ERROR)\n","\n","learning_rate_or = 3e-3 #@param {type: \"number\"}\n","scale_factor = 60000 #@param {type: \"number\"}, scale factor of l2 regularization, default is 3000\n","\n","batchsize = 128 #@param {type: \"integer\"}, batch size, default is 128\n","num_epochs = 6000 #@param {type: \"integer\"}\n","\n","event_path = 'GoogleDrive/My Drive/Colab Notebooks/Tensorboard' #@param {type: \"string\"}\n","checkpoint_path = 'GoogleDrive/My Drive/Colab Notebooks/Checkpoints' #@param {type: \"string\"}\n","online_test = True #@param {type: \"boolean\"}\n"],"execution_count":0,"outputs":[]},{"metadata":{"id":"6ZK2ktl3LbAI","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":84},"outputId":"7f56ac8e-94a5-4a7d-9f51-7e441dad2a30","executionInfo":{"status":"ok","timestamp":1553927053912,"user_tz":-480,"elapsed":1099,"user":{"displayName":"Лянпэн К","photoUrl":"https://lh6.googleusercontent.com/-GXVG-PbMfAw/AAAAAAAAAAI/AAAAAAAAADo/wvm2q-yqQzs/s64/photo.jpg","userId":"04289897042674768581"}}},"cell_type":"code","source":["#@title loading data { display-mode: \"both\" }\n","filepath = 'sample_data/MNIST_data'\n","mnist = input_data.read_data_sets(filepath, one_hot=False)\n","imgs_train, labels_train = mnist.train.images, mnist.train.labels\n","imgs_test, labels_test = mnist.test.images, mnist.test.labels\n","num_samples = labels_test.shape[0]\n","assert imgs_train.max() == 1., \"warning: 'The value of the pixel is preferably between 0 and 1.'\""],"execution_count":4,"outputs":[{"output_type":"stream","text":["Extracting sample_data/MNIST_data/train-images-idx3-ubyte.gz\n","Extracting sample_data/MNIST_data/train-labels-idx1-ubyte.gz\n","Extracting sample_data/MNIST_data/t10k-images-idx3-ubyte.gz\n","Extracting sample_data/MNIST_data/t10k-labels-idx1-ubyte.gz\n"],"name":"stdout"}]},{"metadata":{"id":"bqJ3DfvkL2JU","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":1579},"outputId":"66a75b44-93dc-4e60-c905-4aa89cd9acf5","executionInfo":{"status":"ok","timestamp":1553927125652,"user_tz":-480,"elapsed":65824,"user":{"displayName":"Лянпэн К","photoUrl":"https://lh6.googleusercontent.com/-GXVG-PbMfAw/AAAAAAAAAAI/AAAAAAAAADo/wvm2q-yqQzs/s64/photo.jpg","userId":"04289897042674768581"}}},"cell_type":"code","source":["#@title main programm { display-mode: \"both\" }\n","#@markdown - build the graph\n","graph = tf.Graph()\n","with graph.as_default():\n","    \n","    # Exponential decay learning_rate, attenuation factor 0.9, attenuation step number 100\n","    global_step = tf.Variable(0, name='global_step', trainable=False)\n","    decay_steps = 400\n","    decay_rate = 0.90\n","    learning_rate = tf.train.exponential_decay(learning_rate_or,\n","                                                global_step=global_step,\n","                                                decay_steps=decay_steps,\n","                                                decay_rate=decay_rate,\n","                                                staircase=True,\n","                                                name='learning_rate')\n","    \n","    # \n","    x_p = tf.placeholder(tf.float32, shape=[None, 784], name='input_images')\n","    y_p = tf.placeholder(tf.int64, shape=[None, ], name='labels')\n","    batch_size = tf.placeholder(tf.int64, name='batch_size')\n","    # is_training = tf.placeholder(tf.bool, name='is_training')\n","    data = tf.data.Dataset.from_tensor_slices((x_p, y_p))\n","    # if is_training is not None:\n","    #     data = data.repeat()\n","    #     data_batch = data.shuffle(2).batch(FLAGS.batch_size).prefetch(FLAGS.batch_size)\n","    # else:\n","    #     num_samples = x_p.get_shape().as_list()[0]\n","    #     data_batch = data.batch(num_samples).prefetch(num_samples)\n","    data = data.repeat()\n","    data_batch = data.shuffle(2).batch(batch_size).prefetch(batch_size)\n","    iterator = data_batch.make_initializable_iterator()\n","    \n","    # ------------------input layer-----------------\n","    with tf.name_scope('Input'):\n","        x_input, y_input = iterator.get_next()\n","        y = tf.one_hot(y_input, depth=10)\n","        keep_pro = tf.placeholder(tf.float32)\n","        x_imgs = tf.reshape(x_input, shape=[-1, 28, 28, 1], name='input_images')\n","    \n","    # ------------------conv1 layer-----------------\n","    with tf.name_scope('Conv1'):\n","        w_1 = tf.Variable(tf.truncated_normal([3, 3, 1, 32], stddev=0.1), name='weights_conv1')\n","        b_1 = tf.Variable(tf.constant(0.1, shape=[32]), name='bias_conv1')\n","        h_conv1 = tf.nn.relu(tf.nn.conv2d(x_imgs, w_1, strides=[1, 1, 1, 1], padding='SAME') + b_1)\n","        h_pool1 = tf.nn.max_pool(h_conv1, ksize=[1, 2, 2, 1], strides=[1, 2, 2, 1], padding='SAME')\n","    \n","    # ------------------conv2 layer-----------------\n","    with tf.name_scope('Conv2'):\n","        w_2 = tf.Variable(tf.truncated_normal([3, 3, 32, 64], stddev=0.1), name='weights_conv2')\n","        b_2 = tf.Variable(tf.constant(0.1, shape=[64]), name='bias_conv2')\n","        h_conv2 = tf.nn.relu(tf.nn.conv2d(h_pool1, w_2, strides=[1, 1, 1, 1], padding='SAME') + b_2)\n","        h_pool2 = tf.nn.max_pool(h_conv2, ksize=[1, 2, 2, 1], strides=[1, 2, 2, 1], padding='SAME')\n","        # layer_shape = h_pool2.get_shape().as_list()\n","        # num_f = reduce(lambda a,b:a * b, layer_shape[1:])\n","        # h_pool2_fla = tf.reshape(h_pool2, shape=[-1, num_f])\n","    \n","    # ------------------fc1 layer-----------------\n","    with tf.name_scope('FC1'):\n","        h_pool2_fla = tf.layers.flatten(h_pool2)\n","        num_f = h_pool2_fla.get_shape().as_list()[-1]\n","    \n","        w_fc1 = tf.Variable(tf.truncated_normal([num_f, 128], stddev=0.1), name='weights_fc1')\n","        b_fc1 = tf.Variable(tf.constant(0.1, shape=[128]), name='bias_fc1')\n","        h_fc1 = tf.nn.relu(tf.matmul(h_pool2_fla, w_fc1) + b_fc1)\n","        h_drop1 = tf.nn.dropout(h_fc1, keep_prob=keep_pro, name='Dropout')\n","    \n","    # ------------------output layer-----------------\n","    with tf.name_scope('Output'):\n","        w_fc2 = tf.Variable(tf.truncated_normal([128, 10], stddev=0.1), name='weights_fc2')\n","        b_fc2 = tf.Variable(tf.constant(0.1, shape=[10]), name='bias_fc2')\n","        h_fc2 = tf.matmul(h_drop1, w_fc2) + b_fc2\n","\n","#@markdown - use l2 regularization\n","    # ---------------------regularization_L2----------------\n","    tf.add_to_collection(tf.GraphKeys.WEIGHTS, w_fc1)\n","    tf.add_to_collection(tf.GraphKeys.WEIGHTS, w_fc2)\n","    regularizer = tf.contrib.layers.l2_regularizer(scale=15./scale_factor)\n","    reg_tem = tf.contrib.layers.apply_regularization(regularizer)\n","    with tf.name_scope('loss'):\n","        entropy_loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(labels=y, logits=h_fc2))\n","        entropy_loss = tf.add(entropy_loss, reg_tem, name='l2_loss')\n","    \n","    with tf.name_scope('accuracy'):\n","        prediction = tf.cast(tf.equal(tf.arg_max(h_fc2, 1), tf.argmax(y, 1)), \"float\")\n","        accuracy = tf.reduce_mean(prediction)\n","    \n","    with tf.name_scope('train'):\n","        optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate)\n","        train_op = optimizer.minimize(entropy_loss, global_step=global_step)\n","    \n","    # ----------------------summaries-----------------------\n","    tf.summary.image('input_images', x_imgs, max_outputs=3, collections=['train', 'test'])\n","    tf.summary.histogram('conv1_weights', w_1, collections=['train'])\n","    tf.summary.histogram('conv1_bias', b_1, collections=['train'])\n","    tf.summary.scalar('loss', entropy_loss, collections=['train', 'test'])\n","    tf.summary.scalar('accuracy', accuracy, collections=['train', 'test'])\n","    tf.summary.scalar('global_step', global_step, collections=['train'])\n","    tf.summary.scalar('learning_rate', learning_rate, collections=['train'])\n","    summ_train = tf.summary.merge_all('train')\n","    summ_test = tf.summary.merge_all('test')\n","    \n","    #@markdown - start the session\n","    max_acc = 100.\n","    min_loss = 0.1\n","    sess = tf.Session(graph=graph)\n","    with sess.as_default():\n","        sess.run(tf.global_variables_initializer())\n","        sess.run(iterator.initializer, feed_dict={x_p: imgs_train, y_p: labels_train, batch_size: batchsize})\n","        summ_train_dir = os.path.join(event_path, 'summaries','train')\n","        summ_train_Writer = tf.summary.FileWriter(summ_train_dir)\n","        summ_train_Writer.add_graph(sess.graph)\n","        summ_test_dir = os.path.join(event_path, 'summaries', 'test')\n","        summ_test_Writer = tf.summary.FileWriter(summ_test_dir)\n","        summ_test_Writer.add_graph(sess.graph)\n","        saver = tf.train.Saver(max_to_keep=1)\n","        print(' Training ========== (。・`ω´・) ========')\n","        for num in range(num_epochs):\n","            _, acc, loss, rs = sess.run([train_op, accuracy, entropy_loss, summ_train], feed_dict={keep_pro: 0.5, \n","                                                                                                    batch_size: batchsize})\n","            summ_train_Writer.add_summary(rs, global_step=num)\n","            acc *= 100\n","            num_e = str(num + 1)\n","            print_list = [num_e, loss, acc]\n","            if (num + 1) % 400 == 0:\n","                print('Keep on training ========== (。・`ω´・) ========')\n","                # print(b_x.shape)\n","                # print(b_t)\n","                print('Epoch {0[0]}, train_loss is {0[1]:.4f}, accuracy is {0[2]:.2f}%.\\n'.format(print_list))\n","                \n","                if online_test:\n","                    print(' '*12, 'Online-Testing ========== (。・`ω´・) ========')\n","                    sess.run(iterator.initializer, feed_dict={x_p: imgs_test, y_p: labels_test, batch_size: num_samples})\n","                    acc, loss, rs = sess.run([accuracy, entropy_loss, summ_test], feed_dict={keep_pro: 1., \n","                                                                                                batch_size: num_samples})\n","                    summ_test_Writer.add_summary(rs, global_step=num)\n","                    acc *= 100\n","                    print_list = [num_e, loss, acc]\n","                    # print(' '*10, b_x.shape)\n","                    # print(' '*10, b_t)\n","                    print(' '*10, 'Epoch {0[0]}, test_loss is {0[1]:.4f}, accuracy is {0[2]:.2f}%.\\n'.format(print_list))\n","                    \n","                    sess.run(iterator.initializer, feed_dict={x_p: imgs_train, y_p: labels_train, batch_size: batchsize})\n","    print('\\n', 'Training completed.')\n","    summ_train_Writer.close()\n","    summ_test_Writer.close()\n","    sess.close()"],"execution_count":5,"outputs":[{"output_type":"stream","text":[" Training ========== (。・`ω´・) ========\n","Keep on training ========== (。・`ω´・) ========\n","Epoch 400, train_loss is 0.2496, accuracy is 93.75%.\n","\n","             Online-Testing ========== (。・`ω´・) ========\n","           Epoch 400, test_loss is 0.1912, accuracy is 97.05%.\n","\n","Keep on training ========== (。・`ω´・) ========\n","Epoch 800, train_loss is 0.1336, accuracy is 99.22%.\n","\n","             Online-Testing ========== (。・`ω´・) ========\n","           Epoch 800, test_loss is 0.1204, accuracy is 98.28%.\n","\n","Keep on training ========== (。・`ω´・) ========\n","Epoch 1200, train_loss is 0.1004, accuracy is 99.22%.\n","\n","             Online-Testing ========== (。・`ω´・) ========\n","           Epoch 1200, test_loss is 0.0966, accuracy is 98.67%.\n","\n","Keep on training ========== (。・`ω´・) ========\n","Epoch 1600, train_loss is 0.0805, accuracy is 99.22%.\n","\n","             Online-Testing ========== (。・`ω´・) ========\n","           Epoch 1600, test_loss is 0.0872, accuracy is 98.73%.\n","\n","Keep on training ========== (。・`ω´・) ========\n","Epoch 2000, train_loss is 0.1105, accuracy is 97.66%.\n","\n","             Online-Testing ========== (。・`ω´・) ========\n","           Epoch 2000, test_loss is 0.0790, accuracy is 98.89%.\n","\n","Keep on training ========== (。・`ω´・) ========\n","Epoch 2400, train_loss is 0.0733, accuracy is 98.44%.\n","\n","             Online-Testing ========== (。・`ω´・) ========\n","           Epoch 2400, test_loss is 0.0726, accuracy is 99.05%.\n","\n","Keep on training ========== (。・`ω´・) ========\n","Epoch 2800, train_loss is 0.0717, accuracy is 99.22%.\n","\n","             Online-Testing ========== (。・`ω´・) ========\n","           Epoch 2800, test_loss is 0.0691, accuracy is 99.04%.\n","\n","Keep on training ========== (。・`ω´・) ========\n","Epoch 3200, train_loss is 0.0510, accuracy is 100.00%.\n","\n","             Online-Testing ========== (。・`ω´・) ========\n","           Epoch 3200, test_loss is 0.0695, accuracy is 99.00%.\n","\n","Keep on training ========== (。・`ω´・) ========\n","Epoch 3600, train_loss is 0.0545, accuracy is 99.22%.\n","\n","             Online-Testing ========== (。・`ω´・) ========\n","           Epoch 3600, test_loss is 0.0642, accuracy is 99.08%.\n","\n","Keep on training ========== (。・`ω´・) ========\n","Epoch 4000, train_loss is 0.0527, accuracy is 99.22%.\n","\n","             Online-Testing ========== (。・`ω´・) ========\n","           Epoch 4000, test_loss is 0.0643, accuracy is 98.93%.\n","\n","Keep on training ========== (。・`ω´・) ========\n","Epoch 4400, train_loss is 0.0404, accuracy is 100.00%.\n","\n","             Online-Testing ========== (。・`ω´・) ========\n","           Epoch 4400, test_loss is 0.0592, accuracy is 99.07%.\n","\n","Keep on training ========== (。・`ω´・) ========\n","Epoch 4800, train_loss is 0.0486, accuracy is 99.22%.\n","\n","             Online-Testing ========== (。・`ω´・) ========\n","           Epoch 4800, test_loss is 0.0578, accuracy is 99.19%.\n","\n","Keep on training ========== (。・`ω´・) ========\n","Epoch 5200, train_loss is 0.0405, accuracy is 100.00%.\n","\n","             Online-Testing ========== (。・`ω´・) ========\n","           Epoch 5200, test_loss is 0.0553, accuracy is 99.17%.\n","\n","Keep on training ========== (。・`ω´・) ========\n","Epoch 5600, train_loss is 0.0584, accuracy is 99.22%.\n","\n","             Online-Testing ========== (。・`ω´・) ========\n","           Epoch 5600, test_loss is 0.0562, accuracy is 99.18%.\n","\n","Keep on training ========== (。・`ω´・) ========\n","Epoch 6000, train_loss is 0.0469, accuracy is 99.22%.\n","\n","             Online-Testing ========== (。・`ω´・) ========\n","           Epoch 6000, test_loss is 0.0536, accuracy is 99.20%.\n","\n","\n"," Training completed.\n"],"name":"stdout"}]},{"metadata":{"id":"6Gn5A1H-L24o","colab_type":"code","colab":{}},"cell_type":"code","source":["! fusermount -u 'GoogleDrive'"],"execution_count":0,"outputs":[]}]}